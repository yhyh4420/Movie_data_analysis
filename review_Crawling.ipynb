{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import lxml.html\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from contextlib import ExitStack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e01b12",
   "metadata": {},
   "source": [
    "본 리뷰 크롤링 기본개념\n",
    "1. 네이버 영화에서 크롤링하고 싶었지만 서비스종료로 불가하므로 다음 영화에서 가져오도록 한다.\n",
    "2. 정적 크롤링을 통해 beautifulSoap만을 이용해 하고 싶었지만 다음 영화가 동적 크롤링이 필요하여 Selenium을 사용한다.\n",
    "   1) 영화 상세페이지 url를 먼저 크롤링한 후 movie_info에 있는 영화만 다시 선별\n",
    "   2) url 모음 딕셔너리 활용 리뷰 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe83ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open('C:\\\\Users\\\\yehun chang\\\\Desktop\\\\빅데이터 예측분석\\\\2_기말고사 과제\\\\영화\\\\movieInfo.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "title_data = pd.read_csv('C:\\\\Users\\\\yehun chang\\\\Desktop\\\\빅데이터 예측분석\\\\2_기말고사 과제\\\\영화\\\\movieInfo.csv', encoding=result['encoding'])\n",
    "movie_names = title_data['영화명'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_movie_links(name):\n",
    "    # 웹을 로드하지 않고 크롤링\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    links = []  \n",
    "    \n",
    "    try:\n",
    "        driver.get(\"https://movie.daum.net/main\")  # 다음 영화 메인페이지 \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "        \n",
    "        search_box = driver.find_element(By.NAME, \"q\")  # 검색창 찾기\n",
    "        search_box.send_keys(name)\n",
    "        search_box.send_keys(Keys.RETURN)  # 영화제목 검색\n",
    "\n",
    "        # 영화 탭 검색\n",
    "        movie_tab_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[@data-search-type='movie']\")))\n",
    "\n",
    "        driver.execute_script(\"arguments[0].click();\", movie_tab_button)\n",
    "\n",
    "        try:\n",
    "            # movie link가 하나라도 검색될때까지 대기\n",
    "            movie_link = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/moviedb/main?movieId=')]\")))\n",
    "        except TimeoutException:\n",
    "            # movie link가 없으면 다음 문구 출력\n",
    "            print(f\"TimeoutException: No movie link found for {name}\")\n",
    "            return None\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        movie_items = soup.select('ul.list_searchresult li')\n",
    "        for item in movie_items:\n",
    "            title_tag = item.select_one('.tit_item a.link_tit')\n",
    "            title = title_tag.text.strip() if title_tag else 'No movie title'\n",
    "\n",
    "            link_tag = item.select_one('.tit_item a.link_tit')\n",
    "            link = 'https://movie.daum.net' + link_tag['href'] if link_tag else None\n",
    "\n",
    "            links.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "            })\n",
    "        print(f'크롤링 완료 : {name}')\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return links  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e743e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # 시작시간 기록\n",
    "link_dict = []\n",
    "\n",
    "for item in movie_names:\n",
    "    links = crawl_movie_links(item)\n",
    "    if links:\n",
    "        link_dict.extend(links)\n",
    "    print(link_dict)\n",
    "\n",
    "end_time = time.time()  # 종료시간 기록\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for code execution: {elapsed_time} seconds\")  # 총 소요시간 기록\n",
    "print(link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dict_uniq = [dict(t) for t in {tuple(d.items()) for d in link_dict}]\n",
    "for item in link_dict_uniq:\n",
    "    item['link'] = item['link'].replace('main?', 'grade?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9972f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_movie = [movie for movie in link_dict_uniq if movie['title'] in movie_names]\n",
    "df_filtered_movie = pd.DataFrame(filtered_movie)\n",
    "df_filtered_movie_drop = df_filtered_movie.drop_duplicates(subset='title', keep='first')\n",
    "movie_dictionary = df_filtered_movie_drop.to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "link_kor = []\n",
    "\n",
    "for item in movie_dictionary:\n",
    "    url = item['link']\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status()  # 요청이 성공적으로 이루어지지 않으면 예외 발생\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        title_tag = soup.find('h3', class_='tit_movie')\n",
    "        title = title_tag.find('span', class_='txt_tit').text.strip() if title_tag else \"영화 제목 없음\"\n",
    "\n",
    "        country_tag = soup.find('dt', string='국가')\n",
    "        country = country_tag.find_next('dd').text.strip() if country_tag else \"국가 정보 없음\"\n",
    "\n",
    "        if country == '한국':\n",
    "            link_kor.append({\n",
    "                'title': title,\n",
    "                'link': item['link']\n",
    "            })\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(\"HTTP Error:\", errh)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(\"Error Connecting:\", errc)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(\"Timeout Error:\", errt)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(\"Error:\", err)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_movie_data(movie_info):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        movie_url = movie_info['link']\n",
    "        driver.get(movie_url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        for _ in range(1000):\n",
    "            try:\n",
    "                more_button = driver.find_element(By.XPATH, \"//button[@class='link_fold #more']\")\n",
    "                more_button.click()\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                break\n",
    "\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        review_list = soup.find_all('li', {'id': True})\n",
    "\n",
    "        movie_data = []\n",
    "        for review in review_list:\n",
    "            review_content_tag = review.find('p', {'class': 'desc_txt'})\n",
    "            review_content = review_content_tag.text.strip() if review_content_tag else \"리뷰 내용 없음\"\n",
    "\n",
    "            rating_tag = review.find('div', {'class': 'ratings'})\n",
    "            rating = rating_tag.text.strip() if rating_tag else \"평점 없음\"\n",
    "\n",
    "            timestamp_tag = review.find('span', {'class': 'txt_date'})\n",
    "            timestamp = timestamp_tag.text.strip() if timestamp_tag else \"작성 시간 없음\"\n",
    "\n",
    "            movie_data.append({\n",
    "                'movie_title': movie_info['title'], \n",
    "                'review_contents': review_content,\n",
    "                'review_rating': rating,\n",
    "                'review_timestamp': timestamp\n",
    "            })\n",
    "        print('크롤링 완료')\n",
    "        print(movie_data[:2])\n",
    "        return movie_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{movie_info['title']}에 대해 오류가 발생했습니다: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa448a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_kor 딕셔너리를 기반으로 크롤링 함수를 호출하고 결과를 리스트에 저장\n",
    "def crawl_movie_data_wrapper(movie_info):\n",
    "    result = crawl_movie_data(movie_info)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# WebDriver 인스턴스 생성\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 병렬 크롤링을 위한 ThreadPoolExecutor 설정\n",
    "max_workers = 10\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # 크롤링 함수에 필요한 인자 생성\n",
    "    args_list = link_kor\n",
    "\n",
    "    # 각 영화 페이지를 크롤링 함수에 매핑하여 병렬로 실행\n",
    "    results = list(executor.map(crawl_movie_data_wrapper, args_list))\n",
    "\n",
    "# WebDriver 인스턴스 닫기\n",
    "driver.quit()\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 걸린 시간 출력\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"코드 실행에 소요된 시간: {elapsed_time}초\")\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "movie_data_raw = pd.DataFrame([item for sublist in results if sublist is not None for item in sublist])\n",
    "print(movie_data_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
